Input Layer: Immagine (64x64x3)
    |
    V
Conv2D: 32 filtri, kernel (3x3), ReLU
    |
    V
MaxPooling2D: pool (2x2)
    |
    V
Conv2D: 64 filtri, kernel (3x3), ReLU
    |
    V
MaxPooling2D: pool (2x2)
    |
    V
Flatten
    |
    V
Dropout: 50%
    |
    V
Dense: 128 neuroni, ReLU
    |
    V
Output Layer: 37 neuroni, sigmoid


Funzionamento di Ogni Livello
Convolutional Layers (Conv2D):

Applica filtri convoluzionali all'immagine di input per estrarre caratteristiche locali.
Ogni filtro scorre sull'immagine e produce una mappa di attivazione.
La funzione di attivazione ReLU introduce non-linearità, permettendo alla rete di apprendere rappresentazioni più complesse.
Pooling Layers (MaxPooling2D):

Riduce le dimensioni spaziali delle mappe di attivazione.
Aiuta a ridurre la complessità computazionale e a estrarre caratteristiche invarianti alle traslazioni.
Flatten Layer:

Converte le mappe di attivazione 3D in un vettore 1D.
Questo è necessario per collegare i livelli convoluzionali ai livelli completamente connessi (Dense).
Dropout Layer:

Disattiva casualmente una frazione di neuroni durante l'addestramento.
Aiuta a prevenire l'overfitting migliorando la generalizzazione del modello.
Dense Layers:

Ogni neurone è collegato a tutti i neuroni del livello precedente.
La funzione di attivazione ReLU permette di apprendere rappresentazioni non lineari.
Output Layer:

Utilizza la funzione di attivazione Sigmoid per la regressione multilabel.
Ogni neurone dell'output rappresenta la probabilità di una classe specifica.
Conclusione
Questo modello convoluzionale è progettato per estrarre caratteristiche locali dall'immagine di input attraverso i livelli convoluzionali e di pooling, 
e successivamente utilizzare livelli completamente connessi per effettuare la classificazione. 
L'uso di Dropout aiuta a prevenire l'overfitting, mentre la funzione di attivazione Sigmoid nell'output permette di gestire la regressione multilabel.